---
layout: single
classes:
  - wide
title: Spark - RDD
comments: true
author_profile: true
tags:
  - scala


---

### Spark RDD vs Scala Collections

RDD는 Resilient Distributed Dataset의 약어로서, spark의 분산 컬렉션에 대한 abstraction(Spark's distributed collections abstraction)이며 spark에서 되게 중요한 역할을 합니다.

RDD는 순차적(sequential)이거나 병렬적인(parallel) 변경 불가한 scala collection이랑 비슷해 보일 수 있습니다.

scala의 리스트는 변경 불가한 컬렉션이며 map, flatMap, filter와 같은 higher-order function을 사용할 수 있습니다.

RDD도 마찬가지로, map, flatMap, filter, reduce 같은 비슷한 higher-order function을 사용할 수 있습니다.

RDD 클래스는 물론 더 많은 메소드들이 있겠지만 아래와 같이 간단하게 표현할 수 있습니다.

~~~scala
abstract class RDD[T] {
  def map[U](f: T => U): RDD[U] = ...
  def flatMap[U](f: T => TraversableOnce[U]): RDD[U] = ...
  def filter(f: T => Boolean): RDD[T] = ...
  def reduce(f: (T, T) => T): T = ...
}
~~~

scala collection에 있던 map이나 flatMap 등 여러 higher-order function들이 들어있습니다.

그렇기에 spark의 RDD를 사용하는 것은 scala의 기본적인 순차적, 병렬적 컬렉션을 사용하는 것과 비슷할 수 있습니다.

그러나 RDD의 차이점은 바로 **데이터들이 여러 머신에 걸쳐 퍼져있다**는 것입니다.


### RDD Example - Word Count

spark를 이용해 간단한 예제인 단어 개수 세기를 해 보겠습니다.

먼저, 파일로부터 RDD를 생성합니다. 이 때 rdd의 타입은 RDD[String]입니다.

~~~scala
val rdd = spark.textFile("hdfs://...")
~~~



그리고, flatMap을 이용해 라인들을 단어로 분리합니다.

~~~scala
val count = rdd.flatMap(line => line.split(" "))
~~~



그리고, 모든 단어들이 숫자 1과 함께 있도록 매핑합니다.

~~~scala
val count = rdd.flatMap(line => line.split(" "))
               .map(word => (word, 1))
~~~



마지막으로, word와 쌍을 이루던 1을 모두 합치는 것입니다. 이 때는 reduceByKey를 이용합니다.

~~~scala
val count = rdd.flatMap(line => line.split(" "))
               .map(word => (word, 1))
               .reduceByKey(_ + _)
~~~


이렇게 3줄의 코드만으로도 간결하게 단어 세는 프로그램을 작성할 수 있습니다.

### RDD를 생성하는 방법

RDD를 생성하는 방법에는 크게 두 가지가 있습니다.

* 이미 있는 RDD를 통해 새로운 RDD를 생성(Transfoming an existing RDD)
* `SparkContext` object로부터 생성
  * 최신 버전 spark에서는 `SparkContext`가 `SparkSession`으로 rename되었습니다.

#### 이미 있는 RDD를 통해 새로운 RDD를 생성

scala의 List 컬렉션처럼 map 같은 higher-order function으로 새로운 List를 생성해내는 것과 비슷합니다.

이미 존재하고 있는 RDD에 map이나 flatMap과 같은 higher-order function을 통해 변환된 새로운 RDD를 생성해낼 수 있습니다.



#### `SparkContext`(`SparkSession`) object로부터 생성

이 방법은 되게 중요하다고 제가 본 강의에서도 강조합니다.

SparkContext와 SparkSession이 어떤 객체인지 알고 있어야 합니다.

**왜냐하면 이것들을 통해 spark 클러스터를 다루기 때문입니다.**

새로운 Spark 작업을 시작하라면, SparkContext를 먼저 생성해야 합니다.

Spark 작업에 대한 설정을 하기 위해 몇 가지 파라미터를 넘겨주어야 하고, 그 다음에 이 SparkContext를 이용해 RDD를 만들기 시작합니다.

이 SparkContext는 분산 시스템에 있어서 저희가 고려하기 애매한 부분 등 많은 부분들을 알아서 처리해줍니다.

**저희가 가장 중요하게 기억해야 할 것은 이 SparkContext가 Spark 클러스터를 다룬다는 점입니다.**

SparkContext로 RDD를 생성하는 방법은 두 가지가 있습니다.

먼저, `parallelize` 함수를 사용하는 건데, scala 컬렉션으로부터 새로운 RDD를 생성합니다.

그래서 RDD를 직접 처음부터 생성할 필요는 없습니다. 이미 메모리에 load된 List와 같은 scala 컬렉션이 있다면, 단지 `paralleize` 함수를 통해 List로부터 새로운 RDD를 만들어낼 수 있기 때문입니다.

그 다음으로, `textFile` 함수를 사용할 수 있습니다.

HDFS(하둡 파일 시스템)이나 기본 텍스트 파일을 읽어 String 타입의 RDD를 반환합니다.